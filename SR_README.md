# MoEFsDiC-SR: è¶…åˆ†è¾¨ç‡æ‰©æ•£æ¨¡å‹

åŸºäº MoEFsDiC æ¶æ„çš„é«˜æ•ˆå›¾åƒè¶…åˆ†è¾¨ç‡ç³»ç»Ÿï¼Œåˆ©ç”¨æ··åˆä¸“å®¶ç³»ç»Ÿå’Œé¢‘ç‡åŸŸå¢å¼ºå®ç°å¿«é€Ÿã€é«˜è´¨é‡çš„é«˜æ¸…ç»†èŠ‚é‡å»ºã€‚

## æ ¸å¿ƒç‰¹ç‚¹

### ğŸ¯ SR ä¸“ç”¨è®¾è®¡

1. **LR å›¾åƒæ¡ä»¶è¾“å…¥**
   - LR å›¾åƒä½œä¸ºå¼ºåˆ¶æ€§æ¡ä»¶è¾“å…¥åˆ°æ¯ä¸ª U-Net å—
   - ä¿ç•™ LR å›¾åƒçš„ç»“æ„ä¿¡æ¯ï¼Œå¼•å¯¼ HR é‡å»º

2. **å¤šå°ºåº¦ LR ç‰¹å¾èåˆ**
   - åœ¨ç¼–ç å™¨å’Œè§£ç å™¨çš„æ¯ä¸€å±‚èåˆå¯¹åº”å°ºåº¦çš„ LR ç‰¹å¾
   - è·³è·ƒè¿æ¥ä¸­åŒ…å« LR ä¿¡æ¯

3. **å†…å®¹æ„ŸçŸ¥çš„ MoE è·¯ç”±**
   - Router æ ¹æ® LR å›¾åƒå†…å®¹ï¼ˆè¾¹ç¼˜ã€å¹³æ»‘åŒºåŸŸç­‰ï¼‰åŠ¨æ€é€‰æ‹©ä¸“å®¶
   - ä¸åŒåŒºåŸŸä½¿ç”¨ä¸åŒçš„é‡å»ºç­–ç•¥

4. **é«˜æ•ˆä¸Šé‡‡æ ·**
   - ä½¿ç”¨ Sub-pixel Convolution (Pixel Shuffle)
   - æœ€ç»ˆä¸Šé‡‡æ ·å±‚å®ç°ç©ºé—´åˆ†è¾¨ç‡æå‡

## æ¶æ„å˜æ›´

### ç›¸æ¯”é€šç”¨ MoEFsDiC çš„ä¿®æ”¹

#### 1. Router å¢å¼º
```python
# åŸç‰ˆ
router_logits = Router(x, t_emb, c_emb)

# SR ç‰ˆæœ¬
router_logits = Router(x, t_emb, c_emb, lr_feat)
```

Router ç°åœ¨æ„ŸçŸ¥ï¼š
- æ—¶é—´æ­¥ï¼ˆæ‰©æ•£è¿›åº¦ï¼‰
- LR å›¾åƒç»“æ„ï¼ˆè¾¹ç¼˜ã€çº¹ç†ï¼‰
- æ¡ä»¶ä¿¡æ¯

#### 2. MoE_ConvBlock å¢å¼º
```python
# åŸç‰ˆ
out = MoE_ConvBlock(x, t_emb, c_emb)

# SR ç‰ˆæœ¬
out = MoE_ConvBlock(x, t_emb, c_emb, lr_feat)
```

ç‰¹å¾èåˆæ–¹å¼ï¼š
```python
# æ‹¼æ¥èåˆ
x_fused = torch.cat([x, lr_feat], dim=1)
x_proj = proj_layer(x_fused)
```

#### 3. U-Net æ¶æ„å¢å¼º

**LR ç‰¹å¾é‡‘å­—å¡”**ï¼š
```
LR Image [B, 3, 64, 64]
    â†“ LR Encoder
LR Feat [B, 64, 64, 64]
    â†“ å¤šå°ºåº¦ä¸‹é‡‡æ ·
â”œâ”€ Level 0: [B, 64, 64, 64]   â†’ ç¼–ç å™¨å±‚ 0
â”œâ”€ Level 1: [B, 64, 32, 32]   â†’ ç¼–ç å™¨å±‚ 1  
â”œâ”€ Level 2: [B, 64, 16, 16]   â†’ ç¼–ç å™¨å±‚ 2
â””â”€ Level 3: [B, 64, 8, 8]     â†’ ç“¶é¢ˆå±‚
```

**è·³è·ƒè¿æ¥èåˆ**ï¼š
```
ç¼–ç å™¨è¾“å‡º + LR ç‰¹å¾ â†’ è·³è·ƒè¿æ¥ â†’ è§£ç å™¨
```

**æœ€ç»ˆä¸Šé‡‡æ ·**ï¼š
```
Feature [B, C, 64, 64]
    â†“ Conv + PixelShuffle(4x)
HR Output [B, 3, 256, 256]
```

## ä½¿ç”¨æ–¹æ³•

### 1. è®­ç»ƒ SR æ¨¡å‹

```bash
# ä½¿ç”¨é»˜è®¤ SR é…ç½®
python train_sr.py --config configs/sr_default.yaml --device cuda

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
python train_sr.py --config configs/sr_custom.yaml
```

### 2. å‡†å¤‡æ•°æ®

æ•°æ®é›†ç»“æ„ï¼š
```
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ hr/           # é«˜åˆ†è¾¨ç‡å›¾åƒ
â”‚   â””â”€â”€ lr/           # ä½åˆ†è¾¨ç‡å›¾åƒï¼ˆå¯é€‰ï¼Œä¼šè‡ªåŠ¨ç”Ÿæˆï¼‰
â””â”€â”€ val/
    â”œâ”€â”€ hr/
    â””â”€â”€ lr/
```

æ¨èæ•°æ®é›†ï¼š
- **DIV2K**: 800 å¼  2K åˆ†è¾¨ç‡å›¾åƒ
- **Flickr2K**: 2650 å¼ é«˜è´¨é‡å›¾åƒ
- **Set5, Set14, BSD100**: æ ‡å‡†æµ‹è¯•é›†

### 3. é…ç½®è¯´æ˜

#### SR ä¸“ç”¨é…ç½® (`configs/sr_default.yaml`)

```yaml
sr:
  scale_factor: 4        # è¶…åˆ†å€æ•° (2x/4x/8x)
  lr_channels: 64        # LR ç‰¹å¾é€šé“æ•°

train:
  hr_size: 256          # HR å›¾åƒå°ºå¯¸
  lr_size: 64           # LR å›¾åƒå°ºå¯¸ (hr_size / scale_factor)
  batch_size: 8         # æ‰¹æ¬¡å¤§å°
  learning_rate: 2e-4   # å­¦ä¹ ç‡
```

### 4. æ¨ç†/é‡‡æ ·

```python
import torch
from src.models.moefsndic_sr_unet import MoEFsDiC_SR_UNet

# åŠ è½½æ¨¡å‹
model = MoEFsDiC_SR_UNet(config).cuda()
model.load_state_dict(torch.load('checkpoint.pth'))
model.eval()

# å‡†å¤‡è¾“å…¥
lr_image = load_lr_image()  # [1, 3, 64, 64]
t = torch.tensor([0])       # é‡‡æ ·ä» t=0 å¼€å§‹
c = torch.tensor([0])       # ç±»åˆ«æ¡ä»¶

# ç”Ÿæˆ HR å›¾åƒï¼ˆç®€åŒ–ç¤ºä¾‹ï¼‰
with torch.no_grad():
    # ä»å™ªå£°å¼€å§‹
    hr_noise = torch.randn(1, 3, 256, 256).cuda()
    
    # è¿­ä»£å»å™ª
    for t in reversed(range(num_timesteps)):
        noise_pred, _ = model(hr_noise, torch.tensor([t]), c, lr_image)
        hr_noise = denoise_step(hr_noise, noise_pred, t)
    
    hr_image = hr_noise
```

## æ¶ˆèå®éªŒ

### å®éªŒé…ç½®

| é…ç½® | MoE | Freq | LR Fusion | è¯´æ˜ |
|------|-----|------|-----------|------|
| SR Full | âœ“ | âœ“ | âœ“ | å®Œæ•´ SR æ¨¡å‹ |
| SR No-MoE | âœ— | âœ“ | âœ“ | æ—  MoEï¼Œä½¿ç”¨ Dilated |
| SR No-Freq | âœ“ | âœ— | âœ“ | æ— é¢‘ç‡åŸŸå¢å¼º |
| SR Baseline | âœ— | âœ— | âœ“ | æœ€å° SR åŸºçº¿ |

### è¿è¡Œå®éªŒ

```bash
# å®Œæ•´æ¨¡å‹
python train_sr.py --config configs/sr_default.yaml

# æ—  MoE
python train_sr.py --config configs/sr_no_moe.yaml

# æ— é¢‘ç‡åŸŸ
python train_sr.py --config configs/sr_no_freq.yaml
```

## è®­ç»ƒæŠ€å·§

### 1. æ•°æ®å¢å¼º

```yaml
augmentation:
  random_flip: true      # æ°´å¹³/å‚ç›´ç¿»è½¬
  random_crop: true      # éšæœºè£å‰ª
  color_jitter: false    # é¢œè‰²æŠ–åŠ¨ï¼ˆæ…ç”¨ï¼‰
```

### 2. å­¦ä¹ ç‡è°ƒåº¦

```python
# Cosine Annealing
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, T_max=num_epochs
)

# Warm Restart
scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
    optimizer, T_0=10, T_mult=2
)
```

### 3. æ··åˆç²¾åº¦è®­ç»ƒ

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    noise_pred, moe_logits = model(hr_noisy, t, labels, lr_images)
    loss, dm_loss, load_loss = criterion(noise_pred, noise, moe_logits)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### 4. æ¸è¿›å¼è®­ç»ƒ

```python
# é˜¶æ®µ 1: ä½åˆ†è¾¨ç‡å¿«é€Ÿé¢„è®­ç»ƒ
config['train']['hr_size'] = 128
config['train']['lr_size'] = 32

# é˜¶æ®µ 2: ç›®æ ‡åˆ†è¾¨ç‡ç²¾ç»†è®­ç»ƒ
config['train']['hr_size'] = 256
config['train']['lr_size'] = 64
```

## æ€§èƒ½ä¼˜åŒ–

### æ˜¾å­˜ä¼˜åŒ–

1. **å‡å°‘æ‰¹æ¬¡å¤§å°**
   ```yaml
   batch_size: 4  # ä» 8 é™åˆ° 4
   ```

2. **æ¢¯åº¦ç´¯ç§¯**
   ```python
   accumulation_steps = 4
   for i, batch in enumerate(dataloader):
       loss = compute_loss(batch)
       loss = loss / accumulation_steps
       loss.backward()
       
       if (i + 1) % accumulation_steps == 0:
           optimizer.step()
           optimizer.zero_grad()
   ```

3. **æ¢¯åº¦æ£€æŸ¥ç‚¹**
   ```python
   from torch.utils.checkpoint import checkpoint
   
   # åœ¨æ¨¡å‹ä¸­ä½¿ç”¨
   out = checkpoint(self.conv_block, x, t_emb, c_emb, lr_feat)
   ```

### é€Ÿåº¦ä¼˜åŒ–

1. **ä½¿ç”¨ DS-Conv**
   ```yaml
   use_dsconv_global: true  # å‚æ•°é‡å‡å°‘ ~9 å€
   ```

2. **å‡å°‘ä¸“å®¶æ•°é‡**
   ```yaml
   num_experts: 4   # ä» 8 é™åˆ° 4
   k_active: 1      # ä» 2 é™åˆ° 1
   ```

3. **æ•°æ®åŠ è½½ä¼˜åŒ–**
   ```python
   DataLoader(dataset, num_workers=8, pin_memory=True, prefetch_factor=2)
   ```

## è¯„ä¼°æŒ‡æ ‡

### å›¾åƒè´¨é‡æŒ‡æ ‡

```python
from skimage.metrics import peak_signal_noise_ratio as psnr
from skimage.metrics import structural_similarity as ssim

# PSNR
psnr_value = psnr(hr_gt, hr_pred)

# SSIM
ssim_value = ssim(hr_gt, hr_pred, multichannel=True)

# LPIPS (æ„ŸçŸ¥æŸå¤±)
import lpips
loss_fn = lpips.LPIPS(net='alex')
lpips_value = loss_fn(hr_gt, hr_pred)
```

### MoE è´Ÿè½½åˆ†æ

```python
# åˆ†æä¸“å®¶æ¿€æ´»åˆ†å¸ƒ
expert_activation_counts = torch.zeros(num_experts)
for logits in moe_logits_list:
    topk_indices = torch.topk(logits, k=2, dim=1)[1]
    for idx in topk_indices.flatten():
        expert_activation_counts[idx] += 1

print(f"Expert utilization: {expert_activation_counts / expert_activation_counts.sum()}")
```

## å¸¸è§é—®é¢˜

### Q: SR æ¨¡å‹æ˜¾å­˜æ¶ˆè€—å¤§æ€ä¹ˆåŠï¼Ÿ
A: 
- å‡å°‘ `batch_size` (æ¨è 4-8)
- å‡å°‘ `hr_size` å’Œ `lr_size`
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
- å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ

### Q: ç”Ÿæˆçš„å›¾åƒæœ‰ä¼ªå½±ï¼Ÿ
A: 
- å¢åŠ è®­ç»ƒè½®æ•°
- è°ƒæ•´ `lambda_load`ï¼ˆå¯èƒ½è¿‡å¤§ï¼‰
- æ£€æŸ¥æ•°æ®è´¨é‡
- ä½¿ç”¨æ›´å¹³æ»‘çš„ beta schedule

### Q: MoE è·¯ç”±ä¸å‡è¡¡ï¼Ÿ
A: 
- å¢åŠ  `lambda_load` (0.01 â†’ 0.05)
- å‡å°‘ `num_experts` æˆ–å¢åŠ  `k_active`
- æ£€æŸ¥ LR ç‰¹å¾æ˜¯å¦æ­£ç¡®ä¼ é€’

### Q: å¦‚ä½•å¯è§†åŒ– LR ç‰¹å¾å½±å“ï¼Ÿ
A: 
```python
# ç¦ç”¨ LR ç‰¹å¾
noise_pred_no_lr, _ = model(hr_noisy, t, c, None)

# å¯ç”¨ LR ç‰¹å¾
noise_pred_with_lr, _ = model(hr_noisy, t, c, lr_image)

# å¯è§†åŒ–å·®å¼‚
diff = torch.abs(noise_pred_with_lr - noise_pred_no_lr)
```

## æ‰©å±•æ–¹å‘

### 1. å¤šå°ºåº¦ SR
æ”¯æŒ 2x, 4x, 8x åŒæ—¶è®­ç»ƒï¼š
```python
scale = random.choice([2, 4, 8])
lr_size = hr_size // scale
```

### 2. çœŸå®ä¸–ç•Œé€€åŒ–
æ·»åŠ æ¨¡ç³Šã€å™ªå£°ç­‰çœŸå®é€€åŒ–ï¼š
```python
from degradations import blur_kernel, add_noise
lr_degraded = add_noise(blur_kernel(hr_image))
```

### 3. å‚è€ƒå›¾åƒ SR
ä½¿ç”¨é¢å¤–çš„å‚è€ƒå›¾åƒï¼š
```python
model(hr_noisy, t, c, lr_image, ref_image)
```

### 4. è§†é¢‘ SR
æ‰©å±•åˆ°æ—¶åºä¸€è‡´æ€§ï¼š
```python
# æ·»åŠ æ—¶åºæ³¨æ„åŠ›
temporal_attn = TemporalAttention(channels)
```

## å¼•ç”¨

å¦‚æœä½¿ç”¨æœ¬é¡¹ç›®çš„ SR åŠŸèƒ½ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@software{moefsndic_sr_2025,
  title={MoEFsDiC-SR: Super-Resolution with Mixture-of-Experts Diffusion},
  year={2025}
}
```

## ç›¸å…³èµ„æº

- **åŸå§‹ MoEFsDiC**: `README.md`
- **æ¶æ„è¯¦è§£**: `ARCHITECTURE.md`
- **å¿«é€Ÿå…¥é—¨**: `QUICKSTART.md`
- **SR é…ç½®**: `configs/sr_default.yaml`
- **SR è®­ç»ƒ**: `train_sr.py`

---

**é¡¹ç›®çŠ¶æ€**: âœ… SR åŠŸèƒ½å·²å®Œæˆå¹¶å¯ç”¨

**æœ€åæ›´æ–°**: 2025-10-05

